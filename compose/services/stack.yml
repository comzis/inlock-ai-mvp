networks:
  edge:
    external: true
    name: edge
  mgmt:
    external: true
    name: mgmt
  internal:
    external: true
    name: internal
  socket-proxy:
    external: true
    name: socket-proxy

include:
  - inlock-db.yml
  - inlock-ai.yml
  - ../config/monitoring/logging.yml
  - cockpit-proxy.yml

secrets:
  traefik-basicauth:
    file: /home/comzis/apps/secrets-real/traefik-dashboard-users.htpasswd
  positive_ssl_cert:
    file: /home/comzis/apps/secrets-real/positive-ssl.crt
  positive_ssl_key:
    file: /home/comzis/apps/secrets-real/positive-ssl.key
  portainer_admin_password:
    file: /home/comzis/apps/secrets-real/portainer-admin-password
  n8n_db_password:
    file: /home/comzis/apps/secrets-real/n8n-db-password
  n8n_encryption_key:
    file: /home/comzis/apps/secrets-real/n8n-encryption-key
  grafana_admin_password:
    file: /home/comzis/apps/secrets-real/grafana-admin-password

volumes:
  grafana_data:
  prometheus_data:
  alertmanager_data:

x-default-logging: &default-logging
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"

x-hardening: &hardening
  security_opt:
    - no-new-privileges:true

x-resource-hints: &resource-hints
  deploy:
    resources:
      limits:
        memory: 1g
      reservations:
        memory: 256m

services:
  traefik:
    image: traefik:v3.6.4
    restart: always
    mem_limit: 256m
    mem_reservation: 64m
    deploy:
      resources:
        limits:
          memory: 256m
        reservations:
          memory: 64m
    env_file:
      - /home/comzis/inlock/.env
    environment:
      # Cloudflare token for ACME DNS-01 challenge
      # env_file loads CLOUDFLARE_DNS_API_TOKEN and CLOUDFLARE_API_TOKEN from /home/comzis/inlock/.env
      # Traefik's Cloudflare provider expects CLOUDFLARE_DNS_API_TOKEN
      # The env_file directive above will load these variables automatically
      # No need to explicitly set here - env_file handles it
      - DOMAIN=${DOMAIN}
      - DOCKER_API_VERSION=1.46 # Docker Engine 29+ needs >=1.45 for provider connectivity
      - DOCKER_HOST=tcp://docker-socket-proxy:2375
    command:
      - "--configFile=/etc/traefik/traefik.yml"
    ports:
      - "80:80"
      - "443:443"
      # Metrics port only exposed to mgmt network, not public
      - "127.0.0.1:9100:9100"
    volumes:
      # SECURITY: Removed direct docker.sock mount - using socket-proxy instead
      # - /var/run/docker.sock:/var/run/docker.sock:ro
      - ../../config/traefik/traefik.yml:/etc/traefik/traefik.yml:ro
      - ../../traefik/dynamic:/etc/traefik/dynamic:ro
      - ../../traefik/acme:/etc/traefik/acme
    extra_hosts:
      - "host.docker.internal:host-gateway"
    networks:
      - edge
      - socket-proxy
      - mgmt
    secrets:
      - traefik-basicauth
      - positive_ssl_cert
      - positive_ssl_key
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    tmpfs:
      - /tmp
      - /var/run
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    user: "1000:1000"
    <<: [*hardening, *default-logging]

  oauth2-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.6.0
    restart: always
    mem_limit: 256m
    mem_reservation: 64m
    deploy:
      resources:
        limits:
          memory: 256m
        reservations:
          memory: 64m
    env_file:
      - /home/comzis/inlock/.env
    environment:
      - OAUTH2_PROXY_PROVIDER=oidc
      # env_file loads AUTH0_ADMIN_CLIENT_ID, AUTH0_ADMIN_CLIENT_SECRET, and OAUTH2_PROXY_COOKIE_SECRET
      # We map them to OAUTH2_PROXY_* variables that oauth2-proxy expects
      # Note: Variable substitution requires these to be in shell env when docker-compose runs
      # If not available, env_file will still load them, but we need to map the names
      - OAUTH2_PROXY_CLIENT_ID=${AUTH0_ADMIN_CLIENT_ID:-}
      - OAUTH2_PROXY_CLIENT_SECRET=${AUTH0_ADMIN_CLIENT_SECRET:-}
      - OAUTH2_PROXY_COOKIE_SECRET=${OAUTH2_PROXY_COOKIE_SECRET:-}
      - OAUTH2_PROXY_COOKIE_NAME=inlock_session
      - OAUTH2_PROXY_COOKIE_DOMAIN=.inlock.ai
      - OAUTH2_PROXY_COOKIE_SECURE=true
      - OAUTH2_PROXY_COOKIE_SAMESITE=none
      - OAUTH2_PROXY_EMAIL_DOMAINS=*
      - OAUTH2_PROXY_INSECURE_OIDC_ALLOW_UNVERIFIED_EMAIL=true
      - OAUTH2_PROXY_REDIRECT_URL=https://auth.inlock.ai/oauth2/callback
      - OAUTH2_PROXY_UPSTREAMS=static://202
      - OAUTH2_PROXY_HTTP_ADDRESS=0.0.0.0:4180
      - OAUTH2_PROXY_SKIP_AUTH_PREFLIGHT=true
      - OAUTH2_PROXY_SKIP_AUTH_ROUTES=/oauth2/callback
      - OAUTH2_PROXY_SET_XAUTHREQUEST_REDIRECT=true
      - OAUTH2_PROXY_REVERSE_PROXY=true
      - OAUTH2_PROXY_PASS_HOST_HEADER=true
      - OAUTH2_PROXY_SKIP_REDIRECT_LOOP=true
      - OAUTH2_PROXY_SKIP_OIDC_DISCOVERY=false
      - OAUTH2_PROXY_PROVIDER_DISPLAY_NAME=Auth0
      - OAUTH2_PROXY_SIGNOUT_REDIRECT=https://deploy.inlock.ai
      - OAUTH2_PROXY_METRICS_ADDRESS=0.0.0.0:44180
      - OAUTH2_PROXY_SCOPE=openid profile email
      - OAUTH2_PROXY_OIDC_ISSUER_URL=${AUTH0_ISSUER:-https://comzis.eu.auth0.com/}
      - OAUTH2_PROXY_PASS_ACCESS_TOKEN=true
      - OAUTH2_PROXY_PASS_AUTHORIZATION_HEADER=true
      - OAUTH2_PROXY_SET_XAUTHREQUEST=true
      - OAUTH2_PROXY_SKIP_PROVIDER_BUTTON=true
      - OAUTH2_PROXY_PING_PATH=/ping
    command:
      - --cookie-domain=.inlock.ai
      - --cookie-samesite=none
      - --code-challenge-method=S256
      - --whitelist-domain=.inlock.ai
      - --whitelist-domain=deploy.inlock.ai
      - --whitelist-domain=auth.inlock.ai
      - --whitelist-domain=n8n.inlock.ai
      - --whitelist-domain=dashboard.inlock.ai
      - --whitelist-domain=grafana.inlock.ai
      - --whitelist-domain=cockpit.inlock.ai
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "/bin/oauth2-proxy", "--version"]
      interval: 30s
      timeout: 5s
      retries: 3
    tmpfs:
      - /tmp
    read_only: true
    security_opt:
      - no-new-privileges:true
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  docker-socket-proxy:
    image: ghcr.io/tecnativa/docker-socket-proxy:v0.4.1
    restart: always
    mem_limit: 128m
    mem_reservation: 32m
    deploy:
      resources:
        limits:
          memory: 128m
        reservations:
          memory: 32m
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - CONTAINERS=1
      - EVENTS=1
      - IMAGES=1
      - INFO=1
      - NETWORKS=1
      - PING=1
      - SECRETS=1
      - SERVICES=1
      - SWARM=1
      - TASKS=1
      - VERSION=1
      - VOLUMES=1
    networks:
      - socket-proxy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2375/_ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    read_only: true
    tmpfs:
      - /var/lib/haproxy
      - /var/run
      - /tmp
    cap_drop:
      - ALL
    <<: [*default-logging]

  # Homepage service REMOVED - Replaced by Inlock AI application
  # The Inlock AI app now serves the main domain via Traefik router in routers.yml
  # This prevents maintaining an unused nginx service with stale volumes

  portainer:
    image: portainer/portainer-ce:2.33.5
    restart: always
    mem_limit: 384m
    mem_reservation: 128m
    deploy:
      resources:
        limits:
          memory: 384m
        reservations:
          memory: 128m
    command:
      - "-H"
      - "tcp://docker-socket-proxy:2375"
    volumes:
      - /home/comzis/apps/traefik/portainer_data:/data
    networks:
      - mgmt
      # SECURITY: Removed 'edge' network - access only via Traefik on mgmt network
      - socket-proxy
    secrets:
      - portainer_admin_password
    healthcheck:
      disable: true
    cap_drop:
      - ALL
    user: "1000:1000"
    <<: [*hardening, *default-logging]

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    restart: always
    mem_limit: 384m
    mem_reservation: 128m
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 384m
        reservations:
          memory: 128m
    command:
      - "--disable_metrics=percpu,udp,process,sched"
      - "--housekeeping_interval=10s"
      - "--max_housekeeping_interval=15s"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8080/metrics"]
    read_only: true
    <<: [*hardening, *default-logging]

  prometheus:
    image: prom/prometheus@sha256:d936808bdea528155c0154a922cd42fd75716b8bb7ba302641350f9f3eaeba09
    restart: always
    mem_limit: 1536m
    mem_reservation: 512m
    deploy:
      resources:
        limits:
          cpus: '1'
          memory: 1536m
        reservations:
          memory: 512m
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - prometheus_data:/prometheus
      - ../config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ../config/prometheus/rules.yml:/etc/prometheus/rules.yml:ro
      - ../config/prometheus/rules:/etc/prometheus/rules:ro
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 3
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    <<: [*hardening, *default-logging]

  grafana:
    image: grafana/grafana:11.1.0
    restart: always
    mem_limit: 512m
    mem_reservation: 128m
    deploy:
      resources:
        limits:
          memory: 512m
        reservations:
          memory: 128m
    environment:
      - GF_SERVER_DOMAIN=grafana.inlock.ai
      - GF_SERVER_ROOT_URL=https://grafana.inlock.ai
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=/run/secrets/grafana_admin_password
    volumes:
      - grafana_data:/var/lib/grafana
      - ../../config/grafana/provisioning:/etc/grafana/provisioning:ro
      - ../../config/grafana/dashboards:/etc/grafana/dashboards:ro
    networks:
      - mgmt
      # SECURITY: Removed 'edge' network - access only via Traefik on mgmt network
    secrets:
      - grafana_admin_password
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    <<: [*hardening, *default-logging]

  alertmanager:
    image: prom/alertmanager:v0.27.0
    restart: always
    mem_limit: 256m
    mem_reservation: 64m
    deploy:
      resources:
        limits:
          memory: 256m
        reservations:
          memory: 64m
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
      - "--cluster.listen-address="
    volumes:
      - alertmanager_data:/alertmanager
      - ../config/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/-/ready"]
      interval: 30s
      timeout: 5s
      retries: 3
    <<: [*hardening, *default-logging]

  node-exporter:
    image: prom/node-exporter:v1.8.2
    restart: always
    mem_limit: 128m
    mem_reservation: 32m
    deploy:
      resources:
        limits:
          memory: 128m
        reservations:
          memory: 32m
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    pid: host
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/host:ro,rslave
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9100/metrics"]
      interval: 30s
      timeout: 5s
      retries: 3
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    <<: [*default-logging]

  blackbox-exporter:
    image: prom/blackbox-exporter:v0.27.0
    restart: always
    mem_limit: 128m
    mem_reservation: 32m
    deploy:
      resources:
        limits:
          memory: 128m
        reservations:
          memory: 32m
    command:
      - "--config.file=/etc/blackbox/blackbox.yml"
    volumes:
      - /home/comzis/inlock/compose/config/monitoring/blackbox.yml:/etc/blackbox/blackbox.yml:ro
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9115/metrics"]
      interval: 30s
      timeout: 5s
      retries: 3
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    <<: [*default-logging]

  postgres-exporter:
    image: quay.io/prometheuscommunity/postgres-exporter:v0.15.0
    restart: always
    mem_limit: 128m
    mem_reservation: 32m
    deploy:
      resources:
        limits:
          memory: 128m
        reservations:
          memory: 32m
    environment:
      - DATA_SOURCE_URI=postgres:5432/${N8N_DB:-n8n}?sslmode=disable
      - DATA_SOURCE_USER=${N8N_DB_USER:-n8n}
      - DATA_SOURCE_PASS_FILE=/run/secrets/n8n_db_password
    networks:
      - internal
      - mgmt
    secrets:
      - n8n_db_password
    user: "1000:1000"
    healthcheck:
      test: ["CMD", "/bin/postgres_exporter", "--version"]
    read_only: true
    security_opt:
      - no-new-privileges:true
    <<: [*default-logging]

  # Cockpit - Commented out due to containerization issues
  # The official Cockpit container requires write access to host PAM configuration
  # which conflicts with security best practices. 
  # Recommendation: Run Cockpit on host and access via Traefik proxy
  # 
  # For now, we'll keep Cockpit running on the host system and fix network connectivity
  # cockpit:
  #   image: quay.io/cockpit/ws:latest
  #   restart: always
  #   network_mode: host
  #   volumes:
  #     - /:/host:ro
  #     - /var/run/docker.sock:/var/run/docker.sock:ro
  #   environment:
  #     - COCKPIT_WS_PORT=9090
  #   healthcheck:
  #     test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090"]
  #     interval: 30s
  #     timeout: 5s
  #     retries: 3
  #   cap_add:
  #     - SYS_ADMIN
  #     - NET_ADMIN
  #   security_opt:
  #     - no-new-privileges:false
  #   tmpfs:
  #     - /tmp
  #     - /run
  #     - /var/run
  #   <<: [*default-logging]
