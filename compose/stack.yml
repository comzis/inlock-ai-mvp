networks:
  edge:
    external: true
    name: edge
  mgmt:
    name: mgmt
    driver: bridge
  internal:
    external: true
    name: internal
  socket-proxy:
    external: true
    name: socket-proxy

include:
  - inlock-db.yml
  - inlock-ai.yml
  - logging.yml

secrets:
  traefik-basicauth:
    file: /home/comzis/apps/secrets-real/traefik-dashboard-users.htpasswd
  positive_ssl_cert:
    file: /home/comzis/apps/secrets-real/positive-ssl.crt
  positive_ssl_key:
    file: /home/comzis/apps/secrets-real/positive-ssl.key
  portainer_admin_password:
    file: /home/comzis/apps/secrets-real/portainer-admin-password
  n8n_db_password:
    file: /home/comzis/apps/secrets-real/n8n-db-password
  n8n_encryption_key:
    file: /home/comzis/apps/secrets-real/n8n-encryption-key
  grafana_admin_password:
    file: /home/comzis/apps/secrets-real/grafana-admin-password

volumes:
  grafana_data:
  prometheus_data:
  alertmanager_data:
  vault_data:

x-default-logging: &default-logging
  logging:
    driver: "json-file"
    options:
      max-size: "10m"
      max-file: "3"

x-hardening: &hardening
  security_opt:
    - no-new-privileges:true

x-resource-hints: &resource-hints
  deploy:
    resources:
      limits:
        memory: 1g
      reservations:
        memory: 256m

services:
  traefik:
    image: traefik:v3.6.4
    restart: always
    env_file:
      - ../.env
    environment:
      - CLOUDFLARE_DNS_API_TOKEN=${CLOUDFLARE_API_TOKEN}
      - DOMAIN=${DOMAIN}
      # Docker provider disabled - all routers use file provider
      # DOCKER_API_VERSION not needed since Docker provider is commented out
    command:
      - "--configFile=/etc/traefik/traefik.yml"
    ports:
      - "80:80"
      - "443:443"
      # Metrics port only exposed to mgmt network, not public
      - "127.0.0.1:9100:9100"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ../traefik/traefik.yml:/etc/traefik/traefik.yml:ro
      - ../traefik/dynamic:/etc/traefik/dynamic:ro
      - ../traefik/acme:/etc/traefik/acme
    networks:
      - edge
      - socket-proxy
      - mgmt
    secrets:
      - traefik-basicauth
      - positive_ssl_cert
      - positive_ssl_key
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8080/ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    tmpfs:
      - /tmp
      - /var/run
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    user: "1000:1000"
    <<: [*hardening, *default-logging, *resource-hints]

  docker-socket-proxy:
    image: ghcr.io/tecnativa/docker-socket-proxy:v0.4.1
    restart: always
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      - CONTAINERS=1
      - EVENTS=1
      - IMAGES=1
      - INFO=1
      - NETWORKS=1
      - PING=1
      - SECRETS=1
      - SERVICES=1
      - SWARM=1
      - TASKS=1
      - VERSION=1
      - VOLUMES=1
    networks:
      - socket-proxy
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:2375/_ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    read_only: true
    tmpfs:
      - /var/lib/haproxy
      - /var/run
      - /tmp
    cap_drop:
      - ALL
    <<: [*default-logging]

  # Homepage service REMOVED - Replaced by Inlock AI application
  # The Inlock AI app now serves the main domain via Traefik router in routers.yml
  # This prevents maintaining an unused nginx service with stale volumes

  oauth2-proxy:
    image: quay.io/oauth2-proxy/oauth2-proxy:v7.5.1
    restart: always
    env_file:
      - ../.env
    environment:
      - OAUTH2_PROXY_PROVIDER=oidc
      - OAUTH2_PROXY_OIDC_ISSUER_URL=${AUTH0_ISSUER}
      - OAUTH2_PROXY_CLIENT_ID=${AUTH0_ADMIN_CLIENT_ID}
      - OAUTH2_PROXY_CLIENT_SECRET=${AUTH0_ADMIN_CLIENT_SECRET}
      - OAUTH2_PROXY_COOKIE_SECRET=${OAUTH2_COOKIE_SECRET}
      - OAUTH2_PROXY_EMAIL_DOMAINS=*
      - OAUTH2_PROXY_UPSTREAMS=file:///dev/null
      - OAUTH2_PROXY_SKIP_PROVIDER_BUTTON=true
      - OAUTH2_PROXY_SET_XAUTHREQUEST=true
      - OAUTH2_PROXY_INSECURE_OIDC_ALLOW_UNVERIFIED_EMAIL=true
      - OAUTH2_PROXY_REDIRECT_URL=https://auth.${DOMAIN}/oauth2/callback
      - OAUTH2_PROXY_COOKIE_DOMAIN=.${DOMAIN}
      - OAUTH2_PROXY_SKIP_AUTH_REgex=^/ping$
      - OAUTH2_PROXY_COOKIE_SECURE=true
      - OAUTH2_PROXY_COOKIE_HTTPONLY=true
      - OAUTH2_PROXY_COOKIE_SAMESITE=none
      - OAUTH2_PROXY_COOKIE_CSRF_PER_REQUEST=false
      - OAUTH2_PROXY_HTTP_ADDRESS=0.0.0.0:4180
      - OAUTH2_PROXY_REVERSE_PROXY=true
    networks:
      - mgmt
      - edge
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:4180/ping"]
      interval: 30s
      timeout: 5s
      retries: 3
    cap_drop:
      - ALL
    <<: [*hardening, *default-logging, *resource-hints]

  vault:
    image: hashicorp/vault:1.16.1
    restart: always
    command: server -dev -dev-root-token-id=${VAULT_ROOT_TOKEN:-dev-only-token}
    environment:
      - VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200
      - VAULT_ADDR=http://0.0.0.0:8200
    volumes:
      - vault_data:/vault/file
    networks:
      - mgmt
    ports:
      - "127.0.0.1:8200:8200"  # Vault UI and API only on localhost
    cap_add:
      - IPC_LOCK
    cap_drop:
      - ALL
    <<: [*default-logging, *resource-hints]

  portainer:
    image: portainer/portainer-ce@sha256:c2353b2bd0dbfaa33faa8895222732e8823a4cf5b57ea664ba631ec3e23d842b
    restart: always
    command:
      - "-H"
      - "tcp://docker-socket-proxy:2375"
    volumes:
      - /home/comzis/apps/traefik/portainer_data:/data
    networks:
      - mgmt
      - edge
      - socket-proxy
    secrets:
      - portainer_admin_password
    healthcheck:
      disable: true
    cap_drop:
      - ALL
    user: "1000:1000"
    <<: [*hardening, *default-logging, *resource-hints]

  cadvisor:
    image: gcr.io/cadvisor/cadvisor@sha256:3cde6faf0791ebf7b41d6f8ae7145466fed712ea6f252c935294d2608b1af388
    restart: always
    command:
      - "--disable_metrics=percpu,udp,process,sched"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:8080/metrics"]
    read_only: true
    # NOTE: cAdvisor runs in hardened mode (cap_drop: ALL, read_only, no-new-privileges).
    # This prevents reading /proc/<pid>/smaps for processes in other namespaces, causing
    # warnings about "failed to get smaps" in logs. These warnings are EXPECTED and
    # can be safely ignored. Standard container metrics (memory usage, CPU, etc.) still
    # work correctly. If you need smaps data (per-process RSS/page table breakdown),
    # you must relax the sandboxing: add pid: host and either privileged: true or
    # cap_add: [SYS_ADMIN, SYS_PTRACE]. However, keeping it hardened is recommended
    # for security unless smaps data is specifically required.
    <<: [*hardening, *default-logging, *resource-hints]

  prometheus:
    image: prom/prometheus:v2.51.2@sha256:4f6c47e39a9064028766e8c95890ed15690c30f00c4ba14e7ce6ae1ded0295b1
    restart: always
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=30d'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--web.enable-lifecycle'
    volumes:
      - prometheus_data:/prometheus
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./prometheus/rules:/etc/prometheus/rules:ro
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 5s
      retries: 3
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    <<: [*hardening, *default-logging, *resource-hints]

  grafana:
    image: grafana/grafana@sha256:079600c9517b678c10cda6006b4487d3174512fd4c6cface37df7822756ed7a5
    restart: always
    environment:
      - GF_SERVER_DOMAIN=grafana.${DOMAIN}
      - GF_SERVER_ROOT_URL=https://grafana.${DOMAIN}
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=/run/secrets/grafana_admin_password
    volumes:
      - grafana_data:/var/lib/grafana
      - ../grafana/provisioning:/etc/grafana/provisioning:ro
      - ../grafana/dashboards:/etc/grafana/dashboards:ro
    networks:
      - mgmt
      - edge
    secrets:
      - grafana_admin_password
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 5s
      retries: 3
    tmpfs:
      - /tmp
    cap_drop:
      - ALL
    <<: [*hardening, *default-logging, *resource-hints]

  alertmanager:
    image: prom/alertmanager:v0.27.0
    restart: always
    command:
      - "--config.file=/etc/alertmanager/alertmanager.yml"
      - "--storage.path=/alertmanager"
      - "--cluster.listen-address="
    volumes:
      - alertmanager_data:/alertmanager
      - ./alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9093/-/ready"]
      interval: 30s
      timeout: 5s
      retries: 3
    <<: [*hardening, *default-logging, *resource-hints]

  node-exporter:
    image: prom/node-exporter:v1.8.1
    restart: always
    command:
      - "--path.procfs=/host/proc"
      - "--path.sysfs=/host/sys"
      - "--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)"
    pid: host
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/host:ro,rslave
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9100/metrics"]
      interval: 30s
      timeout: 5s
      retries: 3
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    <<: [*default-logging]

  blackbox-exporter:
    image: prom/blackbox-exporter:v0.25.0
    restart: always
    command:
      - "--config.file=/etc/blackbox/blackbox.yml"
    volumes:
      - ./monitoring/blackbox.yml:/etc/blackbox/blackbox.yml:ro
    networks:
      - mgmt
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:9115/metrics"]
      interval: 30s
      timeout: 5s
      retries: 3
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    read_only: true
    <<: [*default-logging]
